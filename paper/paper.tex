%\documentclass[3p]{elsarticle}
\documentclass[5p]{elsarticle}
\usepackage{natbib,graphicx,amsmath, mhchem,url,colortbl,fancyhdr,subfigure}
\usepackage{multicol}

\journal{EECS 545}
\providecommand{\e}[1]{\ensuremath{\times 10^{#1}}}

\begin{document}
\begin{frontmatter}

%\input{header.tex}
\title{Methods for Polyphonic Music Transcription} % Remove Piano part?
% 'Everything you wanted to know about polyphonic music transcription' 
% Note: according to Scott's final report list, it looks like the best paper according to him is a really thorough survey of an already established field, and he repeatedly indicates that he doesn't necessarily expect any new methods
\author[eadd]{Jeremy Nash}
\ead{nashj@umich.edu}
\author[eadd]{Paul Schroeder}
\ead{pschro@umich.edu}
\address[eadd]{Electrical Engineering Department, University of Michigan, Ann Arbor, MI 48109}
\author[csadd]{Mark Liu}
\ead{markmliu@umich.edu}
\address[csadd]{Computer Science Department, University of Michigan, Ann Arbor, MI 48109}


%\input{abstract.tex}

\begin{abstract}
% 4 sentences - write this last

% State the problem
We present a review of modern methods for polyphonic music transcription.

% Say why it's an interesting problem

% Say what your solution achieves

% Say what follows from your solution


\end{abstract}
\begin{keyword}
Transcription \sep sparse coding \sep non-negative matrix factorization \sep Bayesian non-parametrics \sep music information retrieval
\end{keyword}

\end{frontmatter}

\setcounter{topnumber}{1}

\section{Introduction}


\subsection{Terminology}
Semitones are the smallest music interval in Western music. On a piano, semitones are the adjacent keys.

Timbre represents the spectral envelope, time envelope, noise, tonality, and onset of a musical note. Timbre captures the unique properties of a musical instrument.

Pitches are represented by the notes used in musical notation. Pitches are a psychological perception and not an objective physical property. While we can intuitively assign a linear order to pitches (by referring to pitches as lower or higher than other pitches), pitches are rarely a single frequency and are instead a function of their timbre. 

\section{Motivation}
% No more than one page

% Describe the problem 
%    Use an example to introduce the problem
% State your contributions
%    Bulleted list of contributions


% Explain like you're at a whiteboard

% Here's why people want to do this
% Study of music - understanding music
% recommender systems
% Music information database
% For learning musicians 

% Here's why it's difficult
% Musical timbres
% Time varying - attack, decay, sustain, release
% Number of sources?
% Human perception of pitch
% Musical instrument tuning
% Completely new instruments - software created


% Here's a teaser on how I'm going to solve this problem

% Here's what I'm about to show you

% According to Simon Peyton Jones:
% Here is a problem
% It's an interesting problem
% It's an unsolved problem
% Here is my idea
% My idea works 
% Here's how my idea compares to other approaches


\section{Problem Statement}
% No more than one page

\section{Related Work}
% Contain a table with metrics data, brief description, year published 


\section{Methodologies}
There are currently two main approaches to polyphonic music transcription. The first approach learns a supervised learning model that uses frequency magnitudes to predict the presence of a note. The second approach uses unsupervised learning techniques to extract 

\subsection{Audio data}

% Sampling rate for piano

% STFT
\subsection{Spectrogram}
% Hint at the matrix factorization approach
\subsection{Constant Q transform}

\subsection{Semitone filter bank}
Unlike the linear frequency scale used in an FFT, semitones in Western music are spaced logarithmically. This logarithmic spacing derives from our roughly logarithmic perception of pitches, first captured by \citet{stevens1937scale} through the mel scale. 

Transforming the spectra to a logarithmic frequency scale achieves two main benefits. It reduces the dimensionality of the training data, since most of the spectral energy occupies a logarithmic grid of frequencies. In practice, \citet{bock2012polyphonic} were able to reduce the dimensionality of their training data from 5120 FFT magnitudes down to 183. 

The logarithmic frequency scale also decreases sensitivty to detuning. In a linear frequency scale, detuning, especially in higher semitones, will cause spectral energy to leak into neighboring frequency bins, creating more variability in the training data. On a logarithmic frequency scale, minor detuning would result in less spectral energy leakage and less unnecessary variability in the training data.

\subsection{Note onset detection}
This method proposes restricting the problem to determining note onsets. This is a simpler problem than detecting note durations because of the possibility for time-varying timbres in instruments. Most researchers in this field evaluate their music transcription methods on their note onset detection performance. 
\subsection{Discriminative models}
% SVM, logistic regression
\subsection{SVM}
% Frequency partition

\subsection{Feed forward neural network}

\subsection{Non-negative matrix factorization}

\citet{lee1999learning}
\citet{seung2001algorithms}

\subsection{Sparse non-negative matrix factorization}

One of the great side-effects of NMF is that it often produces a sparse, parts-based representation. This is desirable in music transcription because the piano roll matrix, $H$, is a sparse matrix of note activations. However, the NMF method does not contain an explicit sparsity objective. \citet{hoyer2004non} extended NMF to contain a tunable sparsity objective.

The sparse NMF method in \citet{hoyer2004non} was adopted by \citet{abdallah2004polyphonic} in their transcription system.


% Non-negative Matrix factorization
\subsection{Bayesian nonparametric models}
\subsection{Smoothing}
\subsubsection{HMM smoothing}
\subsubsection{Probabilistic spectral smoothness}
\subsection{Recurrent neural network}

\section{Evaluation}

There is no standard metric to evaluate the performance of a transcription method. However, many researchers use the following methods:

\subsection{MIDI databases} 

\subsection{Evaluation measures}

\subsubsection{}

\subsubsection{Frame-level transcription error score}

\subsection{Method comparison}

\subsubsection{Reported measures}
\begin{center}
\begin{tabular}{|l|l|l|}
\hline
Algorithm & Accuracy & Score \\ \hline
SVM & 21\% & 21 \\ \hline
SVM with HMM & 22\% & 22 \\ \hline
NMF & 23\% & 23 \\ \hline
Sparse NMF & 24\% & 24 \\ \hline
\end{tabular}
\end{center}

\subsubsection{Our evaluation}

The following table captures our evaluation of these methods. We expect these accuracy measures to be lower than they were reported in the original papers because we are using a smaller training dataset and not attempting to optimize the methods. 

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
Algorithm & Accuracy & Score \\ \hline
SVM & 18\% & 18 \\ \hline
SVM with HMM & 19\% & 19 \\ \hline
NMF & 20\% & 20 \\ \hline
Sparse NMF & 21\% & 21 \\ \hline
\end{tabular}
\end{center}

\section{Conclusion}
% Which method is fastest

% Which method has the best performance

% Which one is the simplest method

% Future directions


\section{Individual Effort}
\begin{description}
\item[$\bullet$ Jeremy] wrote and evaluted the SVM method in \citet{poliner2006discriminative}, the Bayesian nonparametric method in \citet{blei2010bayesian}, and the sparse non-negative matrix factorization method in \citet{abdallah2004polyphonic}. Jeremy also wrote the paper.
\item[$\bullet$ Mark] implemented the hidden Markov model in \citet{poliner2006discriminative} and the LSTM network in \citet{bock2012polyphonic}.
\item[$\bullet$ Paul] implemented the LSTM network in \citet{bock2012polyphonic}.
\end{description}

%    Problem statement, both conceptual and mathematical.
%    Motivation: why is the problem important.
%    Related work.
%    Methodologies explored and developed, and their advantages and disadvantages
%    Evaluation: experiments on sythetic and real data, performance measures used, results. I hope to see quantitative, objective evaluations, and comparisons with competitors
%    Conclusions: what did you learn, what were the project's success and failures?
%    Description of individual effort: At the end of the report, in a paragraph (included in the page limit), please include a brief description of each project member's contribution to the project.


% Support vector machines + HMM 

% Honglak Lee's approach: use knn or RBM for input

% Non-negative matrix factorization 

% Supervised NMF

% Combination NMF + svm 

% Recurrent neural network (LSTM)

% Bayesian non-parametric method

% Bayesian non-parametric II (infinite hidden markov model)

% Our approach: Bayesian non parametrics with stochastic ... 




%\input{1.tex}
%\input{fig1.tex}
%\input{2.tex}
%\input{3.tex}
%\input{fig2.tex}
%\input{4.tex}
%\input{5.tex}

% Your project should involve experiments that compare a few different algorithms. These should be well known or state-of-the-art methods, plus possibly any new methods you develop. The experiments should use real (as opposed to simulated) data sets. There is a large repository of machine learning data sets called the ``UCI Machine Learning Repository.'' There are other more domain-specific sources as well that you might find by searching. 



%Your project should focus on a topic that has previously been studied in the machine learning literature. I include this requirement because in the past, some groups have defined entirely new problems for themselves, and been unable to make any progress.
%Your project report should contain an extensive literature review that summarizes fundamental ideas and state-of-the-art methods, with critiques of their strengths and weaknesses. My favorite reports to read are the ones that teach me something interesting.
%You are also encouraged to develop your own methods to solve your chosen problem. 
%Your project should involve experiments that compare a few different algorithms. These should be well known or state-of-the-art methods, plus possibly any new methods you develop. The experiments should use real (as opposed to simulated) data sets. There is a large repository of machine learning data sets called the ``UCI Machine Learning Repository.'' There are other more domain-specific sources as well that you might find by searching. 

%Every report should address the following (but don't feel obligated to make these the section headings)
%    Problem statement, both conceptual and mathematical.
%    Motivation: why is the problem important.
%    Related work.
%    Methodologies explored and developed, and their advantages and disadvantages
%    Evaluation: experiments on sythetic and real data, performance measures used, results. I hope to see quantitative, objective evaluations, and comparisons with competitors
%    Conclusions: what did you learn, what were the project's success and failures?
%    Description of individual effort: At the end of the report, in a paragraph (included in the page limit), please include a brief description of each project member's contribution to the project.


\bibliographystyle{model1-num-names}
\bibliography{transcription}
\end{document}
